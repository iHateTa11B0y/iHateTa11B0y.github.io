{"meta":{"title":"Cowhisper","subtitle":"Toast For The Stupid World","description":null,"author":"Sirius","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tag","date":"2019-09-25T09:56:02.000Z","updated":"2019-09-25T09:58:34.514Z","comments":true,"path":"tag/index.html","permalink":"http://yoursite.com/tag/index.html","excerpt":"","text":""},{"title":"category","date":"2019-09-25T09:57:34.000Z","updated":"2019-09-25T09:57:34.222Z","comments":true,"path":"category/index.html","permalink":"http://yoursite.com/category/index.html","excerpt":"","text":""}],"posts":[{"title":"ROIAlign details & implementation","slug":"ROIAlign-Details-and-Implementation","date":"2019-10-16T02:57:49.000Z","updated":"2019-10-16T03:15:06.838Z","comments":true,"path":"2019/10/16/ROIAlign-Details-and-Implementation/","link":"","permalink":"http://yoursite.com/2019/10/16/ROIAlign-Details-and-Implementation/","excerpt":"","text":"垃圾话环节我也想给大家回顾一下过去展望一下未来，但是我最近真的太忙(lan)了。所以…好好别扔臭鸡蛋了！我说还不行么！ 说ROIAlign之前我们要提一下它的前身ROIPooling。ROIPooling 是存在于two-stage detector两个stage之间的一个特征聚合层。由stage-one 出来的proposal的尺寸往往大小不尽相同，对应的roi feature也是大小不尽相同，这对于后面处理造成很大的麻烦，于是ROIPooling应运而生。通过ROIPooling，roi feature 被池化成相同的大小而后送到stage-two的网络里。 但是ROIPooling也有着自己的弊端，粗略的量化导致了一些不一致的问题(mis-alignment)。所以ROIAlign就是为了解决这些问题。 ROI Pooling 的局限性分析[1]在常见的两级检测框架（比如Fast-RCNN，Faster-RCNN，RFCN）中，ROI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲是浮点数，而池化后的特征图要求尺寸固定。故ROI Pooling这一操作存在两次量化的过程。 将候选框边界量化为整数点坐标值 将量化后的边界区域平均分割成 k x k 个单元(bin),对每一个单元的边界进行量化。 事实上，经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题（misalignment）。 下面我们用直观的例子具体分析一下上述区域不匹配问题。如 图1 所示，这是一个Faster-RCNN检测框架。输入一张$800\\times 800$的图片，图片上有一个$665\\times 665$的包围框(框着一只狗)。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是ROI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7\\times 7$的大小，因此将上述包围框平均分割成$7\\times 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。 ROI Align 的主要思想和具体方法为了解决ROI Pooling的上述缺点，作者提出了ROI Align这一改进的方法(如图ROIAlign_1)。ROI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作，。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如 图ROIAlign_2 所示： 遍历每一个候选区域，保持浮点数边界不做量化 将候选区域分割成k x k个单元，每个单元的边界也不做量化 在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作 这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，ROI Align 在遍历取样点的数量上没有ROIPooling那么多，但却可以获得更好的性能，这主要归功于解决了misalignment的问题。值得一提的是，我在实验时发现，ROI Align在VOC2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受misalignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多。)) Code Refence以FAIR的maskrcnn-benchmark为例，ROIAlign实例化需要三个参数。 12345ROIAlign( output_size, spatial_scale=scale, sampling_ratio=sampling_ratio) 其中spatial_scale是当前feature map相对与原图的scale，sampling_ratio就是每个bin中采样点的个数开平方。 Reference这篇文章大部分篇幅来自一篇我很喜欢的博客，讲的很清晰。欢迎大噶也去看看！侵删。[1]. http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]},{"title":"Evaluation Metrics in Object Detectron","slug":"Evaluation-Metrics-in-Object-Detectron-md","date":"2019-10-15T09:19:46.000Z","updated":"2019-10-15T09:25:51.121Z","comments":true,"path":"2019/10/15/Evaluation-Metrics-in-Object-Detectron-md/","link":"","permalink":"http://yoursite.com/2019/10/15/Evaluation-Metrics-in-Object-Detectron-md/","excerpt":"","text":"垃圾话环节我也想给大家展望一下未来回顾一下过去，但是最近真的很忙(lan)，垃圾话就少说点。今天就写点目标检测的评价指标。 基本概念首先介绍几个基本概念好让我们更好的理解我们今天内容 IOU. Intersection of Union.Just as plain as the name says. 在目标检测任务中，我们用IOU来评价一个预测是不是正确。如果某个预测和GT(groud truth)的IOU大于一个我们设置的阈值，我们就可以把这个预测作为正例(positive)。 TP. 即True Positive。既前景预测正确，预测框和GT框IOU满足阈值条件。 TN. 即True Negative。即背景预测正确。 FP. 即False Positive。即误报，预测了一个不存在物体（一般是检出的物体与GT不满足IOU阈值条件）。 FN.即False Negative。即漏报，检出是背景，但是这里其实是有物体的。 Precision &amp; Recallprecision和recall是两个经常见到的指标 Precisionprecision表示的是所有检出的正例中有多少是GT（预测的准确率）。这里 $t$ 代表的是区分正例(positive)和反例(negative)的阈值。$$precision=\\frac{TP(t)}{TP(t)+FP(t)}$$ Recallrecall表示的是所有GT中有多少被检出了（GT的检测率）。$$recall=\\frac{TP(t)}{TP(t)+FN(t)}$$ APAP读作Average Precision。 由上面Precision和Recall的定义我们可以看出来，我们很难只用Precision或者Recall来描述一个测试集上预测结果的好坏。Prcision高，有可能GT漏检很多（recall低），同样Recall高 precision也可能很低。 所以脱离recall谈precision是没有意义的，反之亦然。所以才会需要AP。下面介绍AP的计算方法。 对于某个category, 我们把检测的置信度从高到低进行排序。 假设这张图片中GT共有M个，我们把Recall所有可能值写出来$(0, \\frac{1}{M}, \\frac{2}{M}, …, 1)$共M+1个。对于其中每个recall值$r_i$我们可以在$recall&gt;r_i$的条件下计算出一个最大的precision $p_{recall&gt;r_i}$。最后对所有算出的precision求均值就是AP。即$$AP=\\frac{1}{M+1} \\sum{p_{recall&gt;r_i}}$$ mAPmAP读作mean Average Precision。别问为啥不是average average precision。我也想问呢。 我们刚刚说的AP是对于某个category, 但是一张图里可能好多category，就categories。所以这个mean指的是所有category AP的mean。 更多的Metrics评价指标由很多，具体选用哪个要根据具体场景需求。例如，如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回。 F1 Score在两者都要求高的情况下，可以用F1来衡量$$ F1 = \\frac{2\\times P\\times R}{P+R}$$","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]},{"title":"How to load an image -- Be well aware of what you load.","slug":"image-load","date":"2019-10-14T07:54:42.000Z","updated":"2019-10-15T09:23:20.069Z","comments":true,"path":"2019/10/14/image-load/","link":"","permalink":"http://yoursite.com/2019/10/14/image-load/","excerpt":"","text":"这篇文章呢主要是记录一些pillow和opencv读图的时候的一些注意事项和常用操作。 为了省事，下面代码块默认已经做过下面两行 12from PIL import Imageimport cv2 读图操作 本地读图 1234* PILimg = Image.open(image_path)* OpenCVcvimg = cv2.imread(image_path) url读图 12345678* PILimport requestsimfile = requests.get(path)Image.open(BytesIO(imfile.content)).convert(&apos;RGB&apos;)* OpenCVimg_nparr = np.fromstring(cv_session.get(url).content, np.uint8)img = cv2.imdecode(img_nparr, cv2.IMREAD_COLOR) 差异 通道：pillow 读出来的图片通道顺序是RGB，OpenCV读出来的通道顺序是BGR。 尺寸：PIL.Image有size成员变量但是没有shape，cv2.imread 读出来的是numpy array没有size只有shape12w,h = pil_image.sizeh,w,c = cv2_image.shape 数据格式转换 PIL.Image to numpy array 12pilimg = Image.open(img_path)cvimg = cv2.cvtColor(numpy.asarray(pilimg),cv2.COLOR_RGB2BGR) numpy array to PIL.Image 123cvimg = cv2.imread(img_path)cvimg = cv2.cvtColor(cvimg,cv2.COLOR_BGR2RGB)pil_img = Image.fromarray(cvimg)","categories":[],"tags":[{"name":"Image Processing","slug":"Image-Processing","permalink":"http://yoursite.com/tags/Image-Processing/"}]},{"title":"Faster RCNN 系列细节笔记","slug":"FasterRCNN_Notes","date":"2019-09-25T12:00:11.000Z","updated":"2019-10-14T08:10:19.731Z","comments":true,"path":"2019/09/25/FasterRCNN_Notes/","link":"","permalink":"http://yoursite.com/2019/09/25/FasterRCNN_Notes/","excerpt":"","text":"Intros最近做了一些debug的工作，比较详细的看了一下Faster RCNN 系列代码，有些细节以前没有理解到位，记录一哈。因为涉及到策略移植的问题，代码是对比着detectron和maskrcnn-benchmark一起看的，下面如果涉及到代码以maskrcnn-benchmark为例（别问为啥，问就是垃圾caffe2）。 Load Weightdetectron训练出来的weight是用pickle存的，存的就是一个numpy array。具体代码 maskrcnnn-benchmark load detectron训练的.pkl文件的时候注意weight里面key的匹配规则。简单的讲是键值处理一下之后进行最长后缀匹配。没有匹配的键值不会报错。一定注意log的信息。 说一句不太相关的批话，数据转换的时候注意精度损失 Backbonebackbone其实没啥好说的，需要注意的有3点。 STRIDE_1x1这个配置是是True表明在resne(x)t的bottleneck module里stride做在最开始的1x1的conv里，False表明stride做在3x3的conv里。 CONV_BODY_FREEZE_AT据说是目标检测的batch size比较小，freeze前两层训练会比较稳定 BN的eps先看下BatchNorm的公式。$$\\frac{\\gamma(x-E[x])}{\\sqrt{Var[x]+eps}} +\\beta = \\frac{\\gamma x}{\\sqrt{Var[x]+eps}} + (\\beta-\\frac{\\gamma E[x]}{\\sqrt{Var[x]+eps}})$$$$=\\hat{\\gamma}x+\\hat{\\beta}$$caffe2存下来的其实bn的weight, bias其实是$\\hat{\\gamma}$和$\\hat{\\beta}$，但是maskrcnn-benchmark load weight的时候是把他们分别load为$\\gamma$和$\\beta$然后running_mean, running_var分别初始化为0, 1 tensor ，这两者在eps=0的时候是完全等价的。 RPN网络结构fpn出来各层的feature map都通过同一个3x3 stride 1的conv后接一个1x1的conv bbox_reg和1x1的conv cls_logits. 123456in_channel = cnum_anchor = n[feature_map_lv_i] --&gt; 3x3 stride1 conv --&gt; 1x1 stride1 conv ([bbox_reg_lv_i]) out_channel=c | out_channel= n*4 |--&gt; 1x1 stride1 conv ([cls_logits_lv_i]) out_channel = n rpn的分类分支只做前后景，所以out_channel = n后接sigmoid就出分了。 生成proposal步骤12345678910111213input: [feature_map_lv_i], [cls_logits_lv_i], [bbox_reg_lv_i] [anchors_lv_i]1. 对于每个level的feature map，feature_map_lv_i(可能有多张图) &gt; cls_logits_lv_i经过sigmoid出objectness_lv_i &gt; 根据objectness_lv_i, pre_nms 取top n &gt; 根据bbox_reg_lv_i，anchor_lv_i decode出来proposal &gt; clip_to_image + remove_small_bbox &gt; nms (thresh=0.7) &gt; post_nms_取top n 得到 [proposal_lv_i]2. [proposal_lv_i] 转换成 [proposal_image_i]3. 最后做一次 post_nms 取top n train的时候对所有proposal做 test的时候对每个image的proposal做4. add_gt_proposal 这步只有train的时候有 Note: 生成anchor的时候会把有部分不在image内的anchor去掉 ROI HEADbox_head网络结构12345num_class = n[feature maps]/[anchors] --&gt; ROIAlgin/ROIPool --&gt; (fc+relu) x 2 --&gt; fc (bbox_reg) | out_channel=n |--&gt; fc (cls_logits) | out_channel=n*4 这里出得分是多分类，softmax出分 生成bbox的过程1231. clip to image2. 对每个类做nms3. 对每张图的bbox取前max_per_image个框 Note: 训练的时候要对proposal subsample保证正负样本比例。 Mask head网络结构1[feature maps]/[proposals] --&gt; ROIAlgin/ROIPool --&gt; 3x3 conv * 4 --&gt; fc Note： 训练的时候只有正样本算loss inference mask 二次线性插值会image原来的size","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://yoursite.com/tags/Papers/"}]},{"title":"Affinity Derivation and Graph Merge for Instance Segmentation","slug":"ADGMIS","date":"2019-06-21T05:02:12.000Z","updated":"2019-10-15T09:22:38.012Z","comments":true,"path":"2019/06/21/ADGMIS/","link":"","permalink":"http://yoursite.com/2019/06/21/ADGMIS/","excerpt":"","text":"Intros这篇文章是由中科大，北航和微软研究院共同完成的。本篇文章提出了一个proposal-free的one-stage实例分割算法。主要思想是通过学习像素点间的仿射关系(affinities)把instance segmentation的任务转化成一个二值问题，即判断两个像素点是不是属于同一个实例的问题。 ApproachStructure本篇文章的提出的实例分割方法主要分为两步，第一步提取语意信息和像素点间的关系，第二部就merge第一步的两个结果，最终达到实例分割的效果。 Step One第一步分为两个branch，Semantic branch和Instance branch。 Semantic branchSemantic branch采用的是SoAT语意分割框架Deeplabv3。也可以换用其他框架 Instance branch 想法：我们的目的是将同一个instance的像素点merge在一起。如果这个物体是只有一块连通区域，我们只要merge两组点对，即$(p(x, y), p(x − 1, y))$和$(p(x, y), p(x , y - 1))$。但由于遮挡instace有时候是碎片化的，我们就需要定义一个检视范围，merge的时候只检视在这个范围里的像素点。我们定义这样一个点的集合是我们的检视范围：$$N(x,y)=\\bigcup_{d\\in D} {N_{d}(x,y)}$$其中$$N_{d}(x,y)={ p(x+a,y+b), \\forall a,b\\in {d,0,-d} \\backslash {p(x,y)}}$$D是距离的集合，本文取$D={1,2,4,8,16,32,64}$结构：由于$N(x,y)$有$7\\times 8=56$个元素，我们指定最后一层的channel数量也是56，对应检视范围内的每个点和中心点是一个instance的可能性。训练训练的时候对于每个像素点生成56维的向量对应检视范围里的56个点。与该像素点是一个instance则置1，反之置0。 Step Two: Graph Merge由Step One我们可以拿到所有像素点的集合$V$和所有像素点间的仿射关系$E$(对应概率矩阵P)，我们可以构成一个加权图$G=(V,E)$。由于点AB之间的仿射关系和点BA之间的仿射关系有相同的意义，所以我们让$e(a,b)=e(b,a)=\\frac{1}{2}(P(a,b)+P(b,a))$，即对原始值取平均值。这样以来G同时又是一个无向图。 Graph Merge简单来说，就是每次寻找最大概率可能是一个instance的点对，把这两个点merge成一个超像素(super-pixel)。而后更新顶点集$V$, 边集$E$。循环整个过程直到选出点对最大概率小于一个阈值($r_\\omega$)。最后把超像素代表的点集拿出来，如果点的个数大于一定阈值($r_c$)则保留这个instance。把这个点集的所有edge的权重求平均就是这个instance的confidence。 Implement Details输入用$32\\times 32$的超像素滑动，只要这个超像素里有前景像素，我们就把这个框认定成前景区域。对所有前景区域拓宽16个像素，再找到tightest bbox作为输入，在本文也叫ROI。 Pixel Affinity Refinement说到现在我们其实还没有用上Semantic branch的信息。Pixel Affinity Refinement要做的事就是利用上Semantic branch的语意信息来重新修正两个点之间的Affinity。很明显如果两个像素点的语意分类不是一类，那他们就不应该被分为同一个instance。从instance branch我们得到一个3维的概率矩阵$P$。其中$P(x,y,c)$代表像素点$p(x,y)$和$p(x_c,y_c)$是一个instance的可能性。这个$P(x,y,c)$是没有Semantic branch提供的信息的。Semantic branch会输出一个有$(m+1)$个channel的概率矩阵，其中$m$是class的个数(m=0代表背景)。$$\\bf{P}(x,y)=(p_0(x,y), p_1(x,y),… ,p_m(x,y))$$我们修正过的Affinity表示为：$$P_r(x,y,c)=\\sigma (\\sum^{m}_{i=1}p_i(x,y)p_1(x_c,y_c))P(x,y,c) $$其中$$\\sigma = 2\\times (\\frac{1}{1+e^{-\\alpha x}} - \\frac{1}{2}) $$可以看出来这个$\\sigma$是从sigmoid函数改过来的，这里文章设$\\alpha=5$来减小语意叉乘的影响。 后来作者发现同一个大类下的子类经常会分类错误，但是大类间基本不会分错。于是他们把同一个大类下的所有类聚合成一个超类(Semantic branch), 并在训练的时候把不同大类的叉乘项强行置零。这几个措施会让结果好看不少。 Resizing ROIROI都rescale到$513\\times 513$ Forcing Local Merge强制在$m\\times m$的局部先进行local merge。局部merge完了再merge其他的像素点。 Performance见论文[https://arxiv.org/abs/1811.10870]","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://yoursite.com/tags/Papers/"}]},{"title":"深度学习里的计算量","slug":"FLOPs-md","date":"2019-06-19T09:01:32.000Z","updated":"2019-10-15T09:23:04.297Z","comments":true,"path":"2019/06/19/FLOPs-md/","link":"","permalink":"http://yoursite.com/2019/06/19/FLOPs-md/","excerpt":"","text":"FLOPS注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。 FLOPs注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度 conv层的计算量计算在不考虑激活层和batch size的情况下$FLOPs = (2\\times C_i \\times K^2 -1) \\times H \\times W \\times C_o$$C_i$ input channel, $K$ kernel size, $H,W$ output feature map size, $C_o$ ouput channel.2是因为MAC算两个op。 在不考虑bias时有-1。理解：$(2\\times C_i \\times K^2 -1) = (C_i \\times K^2) + (C_i \\times K^2 -1)$第一项时乘法运算数， 第二项是加法运算数 FC层计算量计算$FLOPs = (2 \\times I - 1)\\times O$$I$ input neuron number, $O$ output neuron number.","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]}]}