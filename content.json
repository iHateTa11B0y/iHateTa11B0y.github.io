{"meta":{"title":"Cowhisper","subtitle":"Toast For The Stupid World","description":null,"author":"Curtain","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tag","date":"2019-09-25T09:56:02.000Z","updated":"2019-09-25T09:58:34.514Z","comments":true,"path":"tag/index.html","permalink":"http://yoursite.com/tag/index.html","excerpt":"","text":""},{"title":"category","date":"2019-09-25T09:57:34.000Z","updated":"2019-09-25T09:57:34.222Z","comments":true,"path":"category/index.html","permalink":"http://yoursite.com/category/index.html","excerpt":"","text":""}],"posts":[{"title":"Faster RCNN 系列细节笔记","slug":"FasterRCNN_Notes","date":"2019-09-25T12:00:11.000Z","updated":"2019-09-27T04:25:08.785Z","comments":true,"path":"2019/09/25/FasterRCNN_Notes/","link":"","permalink":"http://yoursite.com/2019/09/25/FasterRCNN_Notes/","excerpt":"","text":"Intros最近做了一些debug的工作，比较详细的看了一下Faster RCNN 系列代码，有些细节以前没有理解到位，记录一哈。因为涉及到策略移植的问题，代码是对比着detectron和maskrcnn-benchmark一起看的，下面如果涉及到代码以maskrcnn-benchmark为例（别问为啥，问就是垃圾caffe2）。 Load Weightdetectron训练出来的weight是用pickle存的，存的就是一个numpy array。具体代码 maskrcnnn-benchmark load detectron训练的.pkl文件的时候注意weight里面key的匹配规则。简单的讲是键值处理一下之后进行最长后缀匹配。没有匹配的键值不会报错。一定注意log的信息。 说一句不太相关的批话，数据转换的时候注意精度损失 Backbonebackbone其实没啥好说的，需要注意的有3点。 STRIDE_1x1这个配置是是True表明在resne(x)t的bottleneck module里stride做在最开始的1x1的conv里，False表明stride做在3x3的conv里。 CONV_BODY_FREEZE_AT据说是目标检测的batch size比较小，freeze前两层训练会比较稳定 BN的eps先看下BatchNorm的公式。$$\\frac{\\gamma(x-E[x])}{\\sqrt{Var[x]+eps}} +\\beta = \\frac{\\gamma x}{\\sqrt{Var[x]+eps}} + (\\beta-\\frac{\\gamma E[x]}{\\sqrt{Var[x]+eps}})$$$=\\hat{\\gamma}x+\\hat{\\beta}$caffe2存下来的其实bn的weight, bias其实是$\\hat{\\gamma}$和$\\hat{\\beta}$，但是maskrcnn-benchmark load weight的时候是把他们分别load为$\\gamma$和$\\beta$然后running_mean, running_var分别初始化为0, 1 tensor ，这两者在eps=0的时候是完全等价的。 RPN网络结构fpn出来各层的feature map都通过同一个3x3 stride 1的conv后接一个1x1的conv bbox_reg和1x1的conv cls_logits. 123456in_channel = cnum_anchor = n[feature_map_lv_i] --&gt; 3x3 stride1 conv --&gt; 1x1 stride1 conv ([bbox_reg_lv_i]) out_channel=c | out_channel= n*4 |--&gt; 1x1 stride1 conv ([cls_logits_lv_i]) out_channel = n rpn的分类分支只做前后景，所以out_channel = n后接sigmoid就出分了。 生成proposal步骤12345678910111213input: [feature_map_lv_i], [cls_logits_lv_i], [bbox_reg_lv_i] [anchors_lv_i]1. 对于每个level的feature map，feature_map_lv_i(可能有多张图) &gt; cls_logits_lv_i经过sigmoid出objectness_lv_i &gt; 根据objectness_lv_i, pre_nms 取top n &gt; 根据bbox_reg_lv_i，anchor_lv_i decode出来proposal &gt; clip_to_image + remove_small_bbox &gt; nms (thresh=0.7) &gt; post_nms_取top n 得到 [proposal_lv_i]2. [proposal_lv_i] 转换成 [proposal_image_i]3. 最后做一次 post_nms 取top n train的时候对所有proposal做 test的时候对每个image的proposal做4. add_gt_proposal 这步只有train的时候有 Note: 生成anchor的时候会把有部分不在image内的anchor去掉 ROI HEADbox_head网络结构12345num_class = n[feature maps]/[anchors] --&gt; ROIAlgin/ROIPool --&gt; (fc+relu) x 2 --&gt; fc (bbox_reg) | out_channel=n |--&gt; fc (cls_logits) | out_channel=n*4 这里出得分是多分类，softmax出分 生成bbox的过程1231. clip to image2. 对每个类做nms3. 对每张图的bbox取前max_per_image个框 Note: 训练的时候要对proposal subsample保证正负样本比例。 Mask head网络结构1[feature maps]/[proposals] --&gt; ROIAlgin/ROIPool --&gt; 3x3 conv * 4 --&gt; fc Note： 训练的时候只有正样本算loss inference mask 二次线性插值会image原来的size","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://yoursite.com/tags/Papers/"}]},{"title":"Affinity Derivation and Graph Merge for Instance Segmentation","slug":"ADGMIS","date":"2019-06-21T05:02:12.000Z","updated":"2019-09-25T11:56:14.971Z","comments":true,"path":"2019/06/21/ADGMIS/","link":"","permalink":"http://yoursite.com/2019/06/21/ADGMIS/","excerpt":"","text":"Intros这篇文章是由中科大，北航和微软研究院共同完成的。本篇文章提出了一个proposal-free的one-stage实例分割算法。主要思想是通过学习像素点间的仿射关系(affinities)把instance segmentation的任务转化成一个二值问题，即判断两个像素点是不是属于同一个实例的问题。 ApproachStructure本篇文章的提出的实例分割方法主要分为两步，第一步提取语意信息和像素点间的关系，第二部就merge第一步的两个结果，最终达到实例分割的效果。 Step One第一步分为两个branch，Semantic branch和Instance branch。 Semantic branchSemantic branch采用的是SoAT语意分割框架Deeplabv3。也可以换用其他框架 Instance branch 想法：我们的目的是将同一个instance的像素点merge在一起。如果这个物体是只有一块连通区域，我们只要merge两组点对，即$(p(x, y), p(x − 1, y))$和$(p(x, y), p(x , y - 1))$。但由于遮挡instace有时候是碎片化的，我们就需要定义一个检视范围，merge的时候只检视在这个范围里的像素点。我们定义这样一个点的集合是我们的检视范围：$$N(x,y)=\\bigcup_{d\\in D} {N_{d}(x,y)}$$其中$$N_{d}(x,y)={ p(x+a,y+b), \\forall a,b\\in {d,0,-d} \\backslash {p(x,y)}}$$D是距离的集合，本文取$D={1,2,4,8,16,32,64}$结构：由于$N(x,y)$有$7\\times 8=56$个元素，我们指定最后一层的channel数量也是56，对应检视范围内的每个点和中心点是一个instance的可能性。训练训练的时候对于每个像素点生成56维的向量对应检视范围里的56个点。与该像素点是一个instance则置1，反之置0。 Step Two: Graph Merge由Step One我们可以拿到所有像素点的集合$V$和所有像素点间的仿射关系$E$(对应概率矩阵P)，我们可以构成一个加权图$G=(V,E)$。由于点AB之间的仿射关系和点BA之间的仿射关系有相同的意义，所以我们让$e(a,b)=e(b,a)=\\frac{1}{2}(P(a,b)+P(b,a))$，即对原始值取平均值。这样以来G同时又是一个无向图。 Graph Merge简单来说，就是每次寻找最大概率可能是一个instance的点对，把这两个点merge成一个超像素(super-pixel)。而后更新顶点集$V$, 边集$E$。循环整个过程直到选出点对最大概率小于一个阈值($r_\\omega$)。最后把超像素代表的点集拿出来，如果点的个数大于一定阈值($r_c$)则保留这个instance。把这个点集的所有edge的权重求平均就是这个instance的confidence。 Implement Details输入用$32\\times 32$的超像素滑动，只要这个超像素里有前景像素，我们就把这个框认定成前景区域。对所有前景区域拓宽16个像素，再找到tightest bbox作为输入，在本文也叫ROI。 Pixel Affinity Refinement说到现在我们其实还没有用上Semantic branch的信息。Pixel Affinity Refinement要做的事就是利用上Semantic branch的语意信息来重新修正两个点之间的Affinity。很明显如果两个像素点的语意分类不是一类，那他们就不应该被分为同一个instance。从instance branch我们得到一个3维的概率矩阵$P$。其中$P(x,y,c)$代表像素点$p(x,y)$和$p(x_c,y_c)$是一个instance的可能性。这个$P(x,y,c)$是没有Semantic branch提供的信息的。Semantic branch会输出一个有$(m+1)$个channel的概率矩阵，其中$m$是class的个数(m=0代表背景)。$$\\bf{P}(x,y)=(p_0(x,y), p_1(x,y),… ,p_m(x,y))$$我们修正过的Affinity表示为：$$P_r(x,y,c)=\\sigma (\\sum^{m}_{i=1}p_i(x,y)p_1(x_c,y_c))P(x,y,c) $$其中$$\\sigma = 2\\times (\\frac{1}{1+e^{-\\alpha x}} - \\frac{1}{2}) $$可以看出来这个$\\sigma$是从sigmoid函数改过来的，这里文章设$\\alpha=5$来减小语意叉乘的影响。 后来作者发现同一个大类下的子类经常会分类错误，但是大类间基本不会分错。于是他们把同一个大类下的所有类聚合成一个超类(Semantic branch), 并在训练的时候把不同大类的叉乘项强行置零。这几个措施会让结果好看不少。 Resizing ROIROI都rescale到$513\\times 513$ Forcing Local Merge强制在$m\\times m$的局部先进行local merge。局部merge完了再merge其他的像素点。 Performance见论文[https://arxiv.org/abs/1811.10870]","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://yoursite.com/tags/Papers/"}]},{"title":"深度学习里的计算量","slug":"FLOPs-md","date":"2019-06-19T09:01:32.000Z","updated":"2019-09-25T09:38:00.469Z","comments":true,"path":"2019/06/19/FLOPs-md/","link":"","permalink":"http://yoursite.com/2019/06/19/FLOPs-md/","excerpt":"","text":"FLOPS注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。 FLOPs注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度 conv层的计算量计算在不考虑激活层和batch size的情况下$FLOPs = (2\\times C_i \\times K^2 -1) \\times H \\times W \\times C_o$$C_i$ input channel, $K$ kernel size, $H,W$ output feature map size, $C_o$ ouput channel.2是因为MAC算两个op。 在不考虑bias时有-1。理解：$(2\\times C_i \\times K^2 -1) = (C_i \\times K^2) + (C_i \\times K^2 -1)$第一项时乘法运算数， 第二项是加法运算数 FC层计算量计算$FLOPs = (2 \\times I - 1)\\times O$$I$ input neuron number, $O$ output neuron number.","categories":[],"tags":[{"name":"Deep-Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"}]}]}